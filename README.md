# Natural Language Processing with Deep Learning
Taking together Stanford CS224n course with support of [DeepPavlov](https://deeppavlov.ai/) team.

#### Time: весенний семестр 2020 года, вторник, 19:00

#### Location: Учебный центр 1С, Москва, Дмитровское шоссе, д.9 (метро "Тимирязевская"), аудитория 9235 (2 этаж).

#### News: https://t.me/dlinnlp2020spring

#### Chat: https://t.me/dlinnlp_discuss

#### Forum: https://forum.deeppavlov.ai/c/schools-hackatons/Deep-Learning-in-NLP/41

## Course Structure

* weekly quizes
  * [Quiz 1](https://forms.gle/2Gjgq1ot1dFhQsNZ7)
  * [Quiz 2](https://forms.gle/1kUsvhcmNt7hXsRh7)
  * [Quiz 3](https://forms.gle/zyxKGxpwLi3FANE16)
* up to 5 practical hometasks (Jupyter Notebook), to be announced.
  * [Assignment 1](https://classroom.github.com/a/lU_lW_7H)
  * [Assignment 2](https://classroom.github.com/a/SvJ6u-QK)
  * [Assignment 3](https://classroom.github.com/a/d89zcsa_)
* course project (obligatory), to be announced.

## Spring 2020 syllabus 

### Week 1. Word Vector Representations (11.02.2020)

* [Word Vector Representations (word2vec)](https://youtu.be/8rXD5-xhemo)
* [Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (0:00-38:40, 58:00-1:20:00)
* Additional materials:
  * [CS224n: Natural Language Processing with Deep Learning](https://youtu.be/OQQ-W_63UgQ)
  * [CS224n: Word Vector Representations, word2vec](https://youtu.be/ERibwqs9p38)
  * [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)
  * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
  * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
 
### Week 2. Neural Networks. Backpropagation (18.02.2020)
 
 * [CS224n: Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (38:40-58:00)
 * [Enriching Word Vectors with Subword Information](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051)
 * [CS231n: Backpropagation, Neural Networks 1](https://youtu.be/i94OvYb6noo)
 * Additional materials:
    * [CS224n: GloVe: Global Vectors for Word Representation](https://youtu.be/ASn7ExxLZws)
    * [fastText](https://youtu.be/CHcExDsDeHU)
    
### Week 3. Neural Networks. Initialization and Normalization
 * [CS231n: Neural Networks](https://www.youtube.com/watch?v=gYpoJMlgyXA)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-1)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-2)
  
### Week 4. Neural Networks. Optimization

 * [CS231n: Neural Networks](https://www.youtube.com/watch?v=hd_KFJ5ktUc)
 * [CS231n: Lecture Notes](http://cs231n.github.io/neural-networks-3)

### Week 5. Recurrent Neural Networks and Language Models

 * [CS224n: Language Models and RNNs](https://youtu.be/iWea12EAu6U)
 * [CS224n: Language Models and RNNs: Notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)
 
### Week 6. Vanishing Gradients, Fancy RNNs
* [CS224n: Vanishing Gradients, Fancy RNNs](https://www.youtube.com/watch?v=QEw0qEa0E50&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=7)
* [Paper: On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)
* [Article: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

### Week 7. Convolutional Networks for NLP
* [CS224n: Convolutional Networks for NLP](https://youtu.be/EAJoRA0KX7I)
* [Paper: Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
* [Paper: Comparative Study of CNN and RNN for Natural Language Processing](https://arxiv.org/abs/1702.01923)
* [Paper: Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181.pdf)
   
### Week 8. Translation, Seq2Seq, Attention
* [CS224n: Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=XXtpJxZBa2c)
* [Lecture Notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf)
* [Paper: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
* [Paper: Effective approaches to attention-based neural machine translation](https://arxiv.org/pdf/1508.04025)
* [Article: Attention? Attention! by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
   
## Approximate Syllabus

### Week 6. Deep contextualized word representations

[comment]: <> (1. Deep contextualized word representations, Peters et al., 2018)

[comment]: <> (1. Universal Language Model Fine-tuning for Text Classification, Howard and Ruder, 2018)

[comment]: <> (1. towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f)

[comment]: <> (1. nlp.fast.ai)

[comment]: <> (1. jalammar.github.io/illustrated-bert)

[comment]: <> (https://youtu.be/Lg6MZw_OOLI)

### Week 7. Translation, Seq2Seq, Attention

[comment]: <> (Для подготовки к нему посмотрите вот эту лекцию cs224n:https://youtu.be/7m6noV5-l1E)

[comment]: <> (https://clck.ru/FQ8gR)

[comment]: <> (https://clck.ru/FS497)

### Week 8. Contextual Word Embeddings

### Week 9. Question Answering

### Week 10. Natural Language Generation

## Project Proposals
 * The BERT-based Schema-Guided State Tracking [[paper]](https://arxiv.org/pdf/1909.05855.pdf) [[paper]](https://arxiv.org/pdf/1910.03544.pdf) [[pic]](https://raw.githubusercontent.com/google-research-datasets/dstc8-schema-guided-dialogue/master/schema_guided_overview.png)
 * The BERT Cross-Lingual Transferability [[medium]](https://towardsdatascience.com/bert-based-cross-lingual-question-answering-with-deeppavlov-704242c2ac6f?source=friends_link&sk=b7aef1c29b8a8f067fe62e3bfbea2292) [[paper]](https://arxiv.org/pdf/1906.01502.pdf)
 * BERT adaptation for new languages and tasks [[presentation]](files/main_Huawei.pdf)
 * How conversational is Conversational BERT? [[docs]](http://docs.deeppavlov.ai/en/master/features/models/bert.html)
 * Grammatical error correction [[Shared Task]](https://www.cl.cam.ac.uk/research/nl/bea2019st) [[paper for Russian]](https://arxiv.org/pdf/1910.00353.pdf)
 * Semi-supervised morpheme segmentation [[paper]](https://www.aclweb.org/anthology/W19-4218.pdf)
 * Low-resource morphological inflection [[Shared Task]](https://sigmorphon.github.io/sharedtasks/2018/) (to be updated)
 * More morphological inflection [[Shared Task]](https://sigmorphon.github.io/sharedtasks/2020/)
 * Automatic data augmentation (to be updated)
 * Автоматическое решение ЕГЭ [[Соревнование]](https://ai-journey.ru/competitions)
 * Образовательные приложения глубокого обучения (обучение иностранному или русскому языку)
 * [[Semeval 2020]](http://alt.qcri.org/semeval2020/index.php?id=tasks)
 * [[SemEval 2018]](http://alt.qcri.org/semeval2018/index.php?id=tasks)
 * [[SemEval 2019]](http://alt.qcri.org/semeval2019/index.php?id=tasks)
 * [[Taxonomy enrichment]](https://competitions.codalab.org/competitions/22168)
 * Russian aspect-based sentiment analysis [[Dialog-2015]](http://www.dialog-21.ru/evaluation/2015/sentiment/)


## Related Courses
* CS224n: Natural Language Processing with Deep Learning [[course]](http://web.stanford.edu/class/cs224n/) [[youtube]](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
* CS231n: Convolutional Neural Networks for Visual Recognition [[course]](http://cs231n.stanford.edu/) [[youtube]](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
* [Machine Learning Glossary](https://clck.ru/FFZ2x)
* [Open Machine Learning Course by @yorko](http://mlcourse.ai)
* [DEEP LEARNING НА ПАЛЬЦАХ](http://dlcourse.ai)
* [Theoretical Deep Learning](https://github.com/deepmipt/tdl4)

## Useful Material
* [CS231n: Python & NumPy Tutorial](https://clck.ru/FKKEy)
* [100 numpy exercises](http://github.com/rougier/numpy-100)
* [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528)
* [DeepMath2019](https://www.youtube.com/playlist?list=PLWQvhvMdDChzsThHFe4lYAff3pu2m0v2H)
* [CMU Neural Nets for NLP 2020](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ)

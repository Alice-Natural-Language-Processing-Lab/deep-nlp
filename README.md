# Natural Language Processing with Deep Learning
Taking together Stanford cs224n course with support of [DeepPavlov](https://deeppavlov.ai/) team.

#### Time: весенний семестр 2020 года, вторник, 19:00

#### Location: Учебный центр 1С, .Москва, Дмитровское шоссе, д.9 (метро "Тимирязевская"), аудитория 9235 (2 этаж).

#### News: https://t.me/dlinnlp2020spring

#### Chat: https://t.me/dlinnlp_discuss

#### Forum: https://forum.deeppavlov.ai/c/schools-hackatons/Deep-Learning-in-NLP/41

## Course structure

* weekly quizes
  * [Quiz 1](https://forms.gle/2Gjgq1ot1dFhQsNZ7)
* up to 5 practical hometasks (Jupyter Notebook), to be announced.
* course project (obligatory), to be announced.

## Spring 2020 syllabus 

### Week 1 (11.02.2020)

* [Word Vector Representations (word2vec)](https://youtu.be/8rXD5-xhemo)
* [Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (0:00-38:40, 58:00-1:20:00)
* Additional materials:
  * [CS224n: Natural Language Processing with Deep Learning](https://youtu.be/OQQ-W_63UgQ)
  * [CS224n: Word Vector Representations, word2vec](https://youtu.be/ERibwqs9p38)
  * [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)
  * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
  * [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
 
### Week 2 (18.02.2020)
 
 * [CS224n: Word Vectors and Word Senses](https://youtu.be/kEMJRjEdNzM) (38:40-58:00)
 * [Enriching Word Vectors with Subword Information](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051)
 * Additional materials:
    * [CS224n: GloVe: Global Vectors for Word Representation](https://youtu.be/ASn7ExxLZws)
    * [fastText](https://youtu.be/CHcExDsDeHU)

## Approximate Syllabus
    
### Week 3. Neural Networks. Part 1
  
### Week 4. Neural Networks. Part 2
  
### Week 5. Recurrent Neural Networks and Language Models
  
### Week 6. Deep contextualized word representations
  
### Week 7. Translation, Seq2Seq, Attention

### Week 8. Contextual Word Embeddings

### Week 9. Question Answering

### Week 10. Natural Language Generation

## Related Courses
* CS 224n: Natural Language Processing with Deep Learning [[course]](http://web.stanford.edu/class/cs224n/) [youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
* CS231n: Convolutional Neural Networks for Visual Recognition [course](http://cs231n.stanford.edu/) [youtube](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
* [Machine Learning Glossary](https://clck.ru/FFZ2x)
* [Open Machine Learning Course by @yorko](http://mlcourse.ai)
* [DEEP LEARNING НА ПАЛЬЦАХ](http://dlcourse.ai)
* [Theoretical Deep Learning](https://github.com/deepmipt/tdl4)

## Useful Material
* [CS231n: Python & NumPy Tutorial](https://clck.ru/FKKEy)
* [100 numpy exercises](http://github.com/rougier/numpy-100)
* [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528)
* [DeepMath2019](https://www.youtube.com/playlist?list=PLWQvhvMdDChzsThHFe4lYAff3pu2m0v2H)
* [CMU Neural Nets for NLP 2020](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ)
